{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# *HindiGPT is a Transformer-based model built from scratch to deliver accurate and context-aware English-to-Hindi translations. It utilizes the encoder-decoder architecture and advanced attention mechanisms to capture complex linguistic patterns between English and Hindi.*","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-03-24T16:09:52.645189Z","iopub.execute_input":"2025-03-24T16:09:52.645481Z","iopub.status.idle":"2025-03-24T16:10:01.604939Z","shell.execute_reply.started":"2025-03-24T16:09:52.645451Z","shell.execute_reply":"2025-03-24T16:10:01.603939Z"}}},{"cell_type":"code","source":"!pip install datasets\n!pip install tokenizers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T18:44:58.348023Z","iopub.execute_input":"2025-03-24T18:44:58.348352Z","iopub.status.idle":"2025-03-24T18:45:05.909098Z","shell.execute_reply.started":"2025-03-24T18:44:58.348325Z","shell.execute_reply":"2025-03-24T18:45:05.908255Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.21.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.29.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.12.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.1.31)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Load the data and separate into train, validation and test data","metadata":{}},{"cell_type":"code","source":"import os\nimport math\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom pathlib import Path\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\nos.mkdir(\"./hindigpt\")\nos.mkdir(\"./tokenizer_en\")\nos.mkdir(\"./tokenizer_hi\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrain_dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-hi\", split='train')\nvalidation_dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-hi\", split='validation')\n\nraw_train_dataset, rt_to_skip = random_split(train_dataset, [100000,len(train_dataset)-100000])\nraw_validation_dataset, vt_to_skip = random_split(validation_dataset, [1000,len(validation_dataset)-1000])\n\nprint(f\"Train dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(validation_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T18:45:05.910415Z","iopub.execute_input":"2025-03-24T18:45:05.910672Z","iopub.status.idle":"2025-03-24T18:45:15.921683Z","shell.execute_reply.started":"2025-03-24T18:45:05.910649Z","shell.execute_reply":"2025-03-24T18:45:15.920803Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bea1c807b89642fc8af98c87de74c7b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/259k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d10e896d090a426184f4a4b800357fad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/65.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2e7ae04c8ea43a8b7ab1e85dc551631"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/247k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b6fc858145448c1bd0eeda47ae9ea6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24169f23687342f0afbdf7c49477b627"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/534319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33ab25ac5fe34cc89ef813cdf4d16efa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48f61f97589340008d81e5aa3a471353"}},"metadata":{}},{"name":"stdout","text":"Train dataset size: 534319\nValidation dataset size: 2000\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Create tokenizers","metadata":{}},{"cell_type":"code","source":"from tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\ndef get_ds_iterator(raw_train_dataset, lang):\n  for data in raw_train_dataset:\n    yield data['translation'][lang]\n\n# Create Source Tokenizer - English\ntokenizer_en = Tokenizer(BPE(unk_token=\"[UNK]\"))\ntrainer_en = BpeTrainer(min_frequency=2, special_tokens=[\"[PAD]\",\"[UNK]\",\"[CLS]\", \"[SEP]\", \"[MASK]\"])\ntokenizer_en.pre_tokenizer = Whitespace()\ntokenizer_en.train_from_iterator(get_ds_iterator(raw_train_dataset, \"en\"), trainer=trainer_en)\ntokenizer_en.save(\"./tokenizer_en/tokenizer_en.json\")\n\n# Create Target Tokenizer - Hindi\ntokenizer_hi = Tokenizer(BPE(unk_token=\"[UNK]\"))\ntrainer_hi = BpeTrainer(min_frequency=2, special_tokens=[\"[PAD]\",\"[UNK]\",\"[CLS]\", \"[SEP]\", \"[MASK]\"])\ntokenizer_hi.pre_tokenizer = Whitespace()\ntokenizer_hi.train_from_iterator(get_ds_iterator(raw_train_dataset, \"hi\"), trainer=trainer_hi)\ntokenizer_hi.save(\"./tokenizer_hi/tokenizer_hi.json\")\n\ntokenizer_en = Tokenizer.from_file(\"./tokenizer_en/tokenizer_en.json\")\ntokenizer_hi = Tokenizer.from_file(\"./tokenizer_hi/tokenizer_hi.json\")\n\nsource_vocab_size = tokenizer_en.get_vocab_size()\ntarget_vocab_size = tokenizer_hi.get_vocab_size()\n\nmax_seq_len_source = 0\nmax_seq_len_target = 0\n\nfor data in raw_train_dataset:\n    enc_ids = tokenizer_en.encode(data['translation']['en']).ids\n    dec_ids = tokenizer_hi.encode(data['translation']['hi']).ids\n    max_seq_len_source = max(max_seq_len_source, len(enc_ids))\n    max_seq_len_target = max(max_seq_len_target, len(dec_ids))\n\nprint(f'max_seqlen_source: {max_seq_len_source}')\nprint(f'max_seqlen_target: {max_seq_len_target}')\n\nmax_seq_len = 155","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T18:45:15.923171Z","iopub.execute_input":"2025-03-24T18:45:15.923579Z","iopub.status.idle":"2025-03-24T18:45:43.607266Z","shell.execute_reply.started":"2025-03-24T18:45:15.923557Z","shell.execute_reply":"2025-03-24T18:45:43.606371Z"}},"outputs":[{"name":"stdout","text":"max_seqlen_source: 489\nmax_seqlen_target: 504\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Prepare dataset and dataloader","metadata":{}},{"cell_type":"code","source":"# Transform raw dataset to the encoded dataset that can be processed by the model\nclass EncodeDataset(Dataset):\n    def __init__(self, raw_dataset, max_seq_len):\n        super().__init__()\n        self.raw_dataset = raw_dataset\n        self.max_seq_len = max_seq_len\n\n    def __len__(self):\n        return len(self.raw_dataset)\n\n    def __getitem__(self, index):\n\n        raw_text = self.raw_dataset[index]\n\n        source_text = raw_text['translation']['en']\n        target_text = raw_text['translation']['hi']\n\n        source_text_encoded = tokenizer_en.encode(source_text).ids\n        target_text_encoded = tokenizer_hi.encode(target_text).ids\n\n        CLS_ID = torch.tensor([tokenizer_hi.token_to_id(\"[CLS]\")], dtype=torch.int64)\n        SEP_ID = torch.tensor([tokenizer_hi.token_to_id(\"[SEP]\")], dtype=torch.int64)\n        PAD_ID = tokenizer_hi.token_to_id(\"[PAD]\")\n\n        source_text_encoded = source_text_encoded[:self.max_seq_len - 2]\n        target_text_encoded = target_text_encoded[:self.max_seq_len - 1]\n\n        num_source_padding = max(0, self.max_seq_len - len(source_text_encoded) - 2)\n        num_target_padding = max(0, self.max_seq_len - len(target_text_encoded) - 1)\n\n        encoder_padding = torch.tensor([PAD_ID] * num_source_padding, dtype = torch.int64)\n        decoder_padding = torch.tensor([PAD_ID] * num_target_padding, dtype = torch.int64)\n\n        encoder_input = torch.cat([CLS_ID, torch.tensor(source_text_encoded, dtype=torch.int64), SEP_ID, encoder_padding], dim=0).long()\n\n        decoder_input = torch.cat([CLS_ID, torch.tensor(target_text_encoded, dtype=torch.int64), decoder_padding ], dim=0)\n\n        target_label = torch.cat([torch.tensor(target_text_encoded, dtype=torch.int64),SEP_ID,decoder_padding], dim=0)\n\n        encoder_mask = (encoder_input != PAD_ID).unsqueeze(0).unsqueeze(0).int()\n\n        decoder_mask = (decoder_input != PAD_ID).unsqueeze(0).unsqueeze(0).int() & causal_mask(self.max_seq_len)\n\n        return {\n            'encoder_input': encoder_input,\n            'decoder_input': decoder_input,\n            'target_label': target_label,\n            'encoder_mask': encoder_mask,\n            'decoder_mask': decoder_mask,\n            'source_text': source_text,\n            'target_text': target_text\n        }\n\ndef causal_mask(size):\n        mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n        return mask == 0\n\ntrain_ds = EncodeDataset(raw_train_dataset, max_seq_len)\nval_ds = EncodeDataset(raw_validation_dataset, max_seq_len)\n\ntrain_dataloader = DataLoader(train_ds, batch_size = 32, shuffle = True)\nval_dataloader = DataLoader(val_ds, batch_size = 1, shuffle = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T18:45:43.608307Z","iopub.execute_input":"2025-03-24T18:45:43.608614Z","iopub.status.idle":"2025-03-24T18:45:43.617821Z","shell.execute_reply.started":"2025-03-24T18:45:43.608592Z","shell.execute_reply":"2025-03-24T18:45:43.616834Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Input embedding and positional encoding","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\n\nclass EmbeddingLayer(nn.Module):\n    def __init__(self, d_model: int, vocab_size: int):\n        super().__init__()\n        self.d_model = d_model\n        self.embedding = nn.Embedding(vocab_size, d_model)\n\n    def forward(self, input):\n        embedding_output = self.embedding(input) * math.sqrt(self.d_model)\n        return embedding_output\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, max_seq_len: int, dropout_rate: float):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout_rate)\n        pe = torch.zeros(max_seq_len, d_model)\n\n        pos = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n\n        pe[:, 0::2] = torch.sin(pos * div_term)\n        pe[:, 1::2] = torch.cos(pos * div_term)\n\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, input_embdding):\n        input_embdding = input_embdding + (self.pe[:, :input_embdding.shape[1], :]).requires_grad_(False)\n        return self.dropout(input_embdding)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T18:45:43.618802Z","iopub.execute_input":"2025-03-24T18:45:43.619063Z","iopub.status.idle":"2025-03-24T18:45:43.633430Z","shell.execute_reply.started":"2025-03-24T18:45:43.619029Z","shell.execute_reply":"2025-03-24T18:45:43.632798Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Multihead Attention","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, dropout_rate: float):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout_rate)\n        self.num_heads = num_heads\n        assert d_model % num_heads == 0, \"d_model must be divisible by number of heads\"\n\n        self.d_k = d_model // num_heads\n\n        self.W_q = nn.Linear(d_model, d_model, bias=False)\n        self.W_k = nn.Linear(d_model, d_model, bias=False)\n        self.W_v = nn.Linear(d_model, d_model, bias=False)\n        self.W_o = nn.Linear(d_model, d_model, bias=False)\n\n    def forward(self, q, k, v, encoder_mask):\n        query = self.W_q(q)\n        key = self.W_k(k)\n        value = self.W_v(v)\n        query = query.view(query.shape[0], query.shape[1], self.num_heads ,self.d_k).transpose(1,2)\n        key = key.view(key.shape[0], key.shape[1], self.num_heads ,self.d_k).transpose(1,2)\n        value = value.view(value.shape[0], value.shape[1], self.num_heads ,self.d_k).transpose(1,2)\n\n        # SELF ATTENTION BLOCK STARTS\n\n        attention_score = (query @ key.transpose(-2,-1))/math.sqrt(self.d_k)\n\n        if encoder_mask is not None:\n          attention_score = attention_score.masked_fill(encoder_mask == 0, float('-inf'))\n\n        attention_score = attention_score.softmax(dim=-1)\n\n        if self.dropout is not None:\n          attention_score = self.dropout(attention_score)\n\n        attention_output = attention_score @ value\n\n        # SELF ATTENTION BLOCK ENDS\n\n        attention_output = attention_output.transpose(1,2).contiguous().view(attention_output.shape[0], -1, self.num_heads * self.d_k)\n\n        multihead_output = self.W_o(attention_output)\n\n        return multihead_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T18:45:43.634154Z","iopub.execute_input":"2025-03-24T18:45:43.634443Z","iopub.status.idle":"2025-03-24T18:45:43.645975Z","shell.execute_reply.started":"2025-03-24T18:45:43.634422Z","shell.execute_reply":"2025-03-24T18:45:43.645322Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Feedfoward Network, Layer Normalization and AddAndNorm","metadata":{}},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self, d_model: int, d_ff: int, dropout_rate: float):\n        super().__init__()\n\n        self.dropout = nn.Dropout(dropout_rate)\n        self.layer_1 = nn.Linear(d_model, d_ff)\n        self.layer_2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, input):\n        return self.layer_2(self.dropout(torch.relu(self.layer_1(input))))\n\nclass LayerNorm(nn.Module):\n    def __init__(self, eps: float = 1e-5):\n        super().__init__()\n        self.eps = eps\n        self.gamma = nn.Parameter(torch.ones(512))\n        self.beta = nn.Parameter(torch.zeros(512))\n\n    def forward(self, input):\n        mean = input.mean(dim = -1, keepdim=True)\n        std = input.std(dim = -1, keepdim=True)\n        return self.gamma * (input - mean)/(std + self.eps) + self.beta\n\nclass AddAndNorm(nn.Module):\n  def __init__(self, dropout_rate: float):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout_rate)\n        self.layer_norm = LayerNorm()\n\n  def forward(self, input, sub_layer):\n        return input + self.dropout(sub_layer(self.layer_norm(input)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T18:45:43.646701Z","iopub.execute_input":"2025-03-24T18:45:43.646957Z","iopub.status.idle":"2025-03-24T18:45:43.658145Z","shell.execute_reply.started":"2025-03-24T18:45:43.646926Z","shell.execute_reply":"2025-03-24T18:45:43.657502Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Encoder block and Encoder","metadata":{}},{"cell_type":"code","source":"class EncoderBlock(nn.Module):\n    def __init__(self, multihead_attention: MultiHeadAttention, feed_forward: FeedForward, dropout_rate: float) -> None:\n        super().__init__()\n        self.multihead_attention = multihead_attention\n        self.feed_forward = feed_forward\n        self.addnorm_1 = AddAndNorm(dropout_rate)\n        self.addnorm_2 = AddAndNorm(dropout_rate)\n\n    def forward(self, encoder_input, encoder_mask):\n        encoder_input = self.addnorm_1(encoder_input, lambda encoder_input: self.multihead_attention(encoder_input, encoder_input, encoder_input, encoder_mask))\n        encoder_input = self.addnorm_2(encoder_input, self.feed_forward)\n        return encoder_input\n\nclass Encoder(nn.Module):\n    def __init__(self, encoderblocklist: nn.ModuleList) -> None:\n        super().__init__()\n        self.encoderblocklist = encoderblocklist\n        self.layer_norm = LayerNorm()\n\n    def forward(self, encoder_input, encoder_mask):\n        for encoderblock in self.encoderblocklist:\n            encoder_input = encoderblock(encoder_input, encoder_mask)\n        encoder_output = self.layer_norm(encoder_input)\n        return encoder_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T18:45:43.660543Z","iopub.execute_input":"2025-03-24T18:45:43.660733Z","iopub.status.idle":"2025-03-24T18:45:43.674192Z","shell.execute_reply.started":"2025-03-24T18:45:43.660717Z","shell.execute_reply":"2025-03-24T18:45:43.673530Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Decoder block and decoder and the projection","metadata":{}},{"cell_type":"code","source":"class DecoderBlock(nn.Module):\n    def __init__(self, masked_multihead_attention: MultiHeadAttention, cross_multihead_attention: MultiHeadAttention, feed_forward: FeedForward, dropout_rate: float) -> None:\n        super().__init__()\n        self.masked_multihead_attention = masked_multihead_attention\n        self.cross_multihead_attention = cross_multihead_attention\n        self.feed_forward = feed_forward\n        self.addnorm_1 = AddAndNorm(dropout_rate)\n        self.addnorm_2 = AddAndNorm(dropout_rate)\n        self.addnorm_3 = AddAndNorm(dropout_rate)\n\n    def forward(self, decoder_input, encoder_output, encoder_mask, decoder_mask):\n        decoder_input = self.addnorm_1(decoder_input, lambda decoder_input: self.masked_multihead_attention(decoder_input, decoder_input, decoder_input, decoder_mask))\n        decoder_input = self.addnorm_2(decoder_input, lambda decoder_input: self.cross_multihead_attention(decoder_input, encoder_output, encoder_output, encoder_mask))\n        decoder_input = self.addnorm_3(decoder_input, self.feed_forward)\n        return decoder_input\n\nclass Decoder(nn.Module):\n    def __init__(self, decoderblocklist: nn.ModuleList) -> None:\n        super().__init__()\n        self.decoderblocklist = decoderblocklist\n        self.layer_norm = LayerNorm()\n\n    def forward(self, decoder_input, encoder_output, encoder_mask, decoder_mask):\n        for decoderblock in self.decoderblocklist:\n            decoder_input = decoderblock(decoder_input, encoder_output, encoder_mask, decoder_mask)\n        decoder_output = self.layer_norm(decoder_input)\n        return decoder_output\n\nclass ProjectionLayer(nn.Module):\n    def __init__(self, d_model, vocab_size):\n        super().__init__()\n        self.projection_layer = nn.Linear(d_model, vocab_size)\n\n    def forward(self, decoder_output):\n        return self.projection_layer(decoder_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T18:45:43.675181Z","iopub.execute_input":"2025-03-24T18:45:43.675453Z","iopub.status.idle":"2025-03-24T18:45:43.690733Z","shell.execute_reply.started":"2025-03-24T18:45:43.675434Z","shell.execute_reply":"2025-03-24T18:45:43.689980Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Create and build Transfomer","metadata":{}},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, encoder: Encoder, decoder: Decoder, source_embed: EmbeddingLayer, target_embed: EmbeddingLayer, source_pos: PositionalEncoding, target_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n        super().__init__()\n\n        self.source_embed = source_embed\n        self.source_pos = source_pos\n        self.encoder = encoder\n\n        self.target_embed = target_embed\n        self.target_pos = target_pos\n        self.decoder = decoder\n\n        self.projection_layer = projection_layer\n\n    def encode(self, encoder_input, encoder_mask):\n        encoder_input = self.source_embed(encoder_input)\n        encoder_input = self.source_pos(encoder_input)\n        encoder_output = self.encoder(encoder_input, encoder_mask)\n        return encoder_output\n\n    def decode(self, encoder_output, encoder_mask, decoder_input, decoder_mask):\n        decoder_input = self.target_embed(decoder_input)\n        decoder_input = self.target_pos(decoder_input)\n        decoder_output = self.decoder(decoder_input, encoder_output, encoder_mask, decoder_mask)\n        return decoder_output\n\n    def project(self, decoder_output):\n        return self.projection_layer(decoder_output)\n\ndef build_model(source_vocab_size: int, target_vocab_size: int, source_seq_len: int, target_seq_len: int, d_model: int=512, num_blocks: int=6, num_heads: int=8, dropout_rate: float=0.1, d_ff: int=2048) -> Transformer:\n    # Creating the embedding layers\n    source_embed = EmbeddingLayer(d_model, source_vocab_size)\n    target_embed = EmbeddingLayer(d_model, target_vocab_size)\n\n    # Creating the positional encoding layers\n    source_pos = PositionalEncoding(d_model, source_seq_len, dropout_rate)\n    target_pos = PositionalEncoding(d_model, target_seq_len, dropout_rate)\n\n    # Creating the encoder-block-list\n    encoderblocklist = []\n    for _ in range(num_blocks):\n        multihead_attention = MultiHeadAttention(d_model, num_heads, dropout_rate)\n        feed_forward = FeedForward(d_model, d_ff, dropout_rate)\n        encoder_block = EncoderBlock(multihead_attention, feed_forward, dropout_rate)\n        encoderblocklist.append(encoder_block)\n\n    # Creating the encoder\n    encoder = Encoder(nn.ModuleList(encoderblocklist))\n\n    # Creating the decoder-block-list\n    decoderblocklist = []\n    for _ in range(num_blocks):\n        masked_multihead_attention = MultiHeadAttention(d_model,num_heads, dropout_rate)\n        cross_multihead_attention = MultiHeadAttention(d_model, num_heads, dropout_rate)\n        feed_forward = FeedForward(d_model, d_ff, dropout_rate)\n        decoder_block = DecoderBlock(masked_multihead_attention, cross_multihead_attention, feed_forward, dropout_rate)\n        decoderblocklist.append(decoder_block)\n        \n    # Creating the decoder\n    decoder = Decoder(nn.ModuleList(decoderblocklist))\n\n    # Creating the projection layer\n    projection_layer = ProjectionLayer(d_model, target_vocab_size)\n\n    model = Transformer(encoder, decoder, source_embed, target_embed, source_pos, target_pos, projection_layer)\n\n    for p in model.parameters():\n        if p.dim() > 1:\n            nn.init.xavier_uniform_(p)\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T18:45:43.691489Z","iopub.execute_input":"2025-03-24T18:45:43.691683Z","iopub.status.idle":"2025-03-24T18:45:43.706050Z","shell.execute_reply.started":"2025-03-24T18:45:43.691668Z","shell.execute_reply":"2025-03-24T18:45:43.705436Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# The final model architecture","metadata":{}},{"cell_type":"code","source":"model = build_model(tokenizer_en.get_vocab_size(), tokenizer_hi.get_vocab_size(),max_seq_len, max_seq_len, d_model=512).to(device)\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T18:45:43.706795Z","iopub.execute_input":"2025-03-24T18:45:43.707061Z","iopub.status.idle":"2025-03-24T18:45:45.382229Z","shell.execute_reply.started":"2025-03-24T18:45:43.707042Z","shell.execute_reply":"2025-03-24T18:45:45.381354Z"}},"outputs":[{"name":"stdout","text":"Transformer(\n  (source_embed): EmbeddingLayer(\n    (embedding): Embedding(30000, 512)\n  )\n  (source_pos): PositionalEncoding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): Encoder(\n    (encoderblocklist): ModuleList(\n      (0-5): 6 x EncoderBlock(\n        (multihead_attention): MultiHeadAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (W_q): Linear(in_features=512, out_features=512, bias=False)\n          (W_k): Linear(in_features=512, out_features=512, bias=False)\n          (W_v): Linear(in_features=512, out_features=512, bias=False)\n          (W_o): Linear(in_features=512, out_features=512, bias=False)\n        )\n        (feed_forward): FeedForward(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_1): Linear(in_features=512, out_features=2048, bias=True)\n          (layer_2): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (addnorm_1): AddAndNorm(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm()\n        )\n        (addnorm_2): AddAndNorm(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm()\n        )\n      )\n    )\n    (layer_norm): LayerNorm()\n  )\n  (target_embed): EmbeddingLayer(\n    (embedding): Embedding(30000, 512)\n  )\n  (target_pos): PositionalEncoding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): Decoder(\n    (decoderblocklist): ModuleList(\n      (0-5): 6 x DecoderBlock(\n        (masked_multihead_attention): MultiHeadAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (W_q): Linear(in_features=512, out_features=512, bias=False)\n          (W_k): Linear(in_features=512, out_features=512, bias=False)\n          (W_v): Linear(in_features=512, out_features=512, bias=False)\n          (W_o): Linear(in_features=512, out_features=512, bias=False)\n        )\n        (cross_multihead_attention): MultiHeadAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (W_q): Linear(in_features=512, out_features=512, bias=False)\n          (W_k): Linear(in_features=512, out_features=512, bias=False)\n          (W_v): Linear(in_features=512, out_features=512, bias=False)\n          (W_o): Linear(in_features=512, out_features=512, bias=False)\n        )\n        (feed_forward): FeedForward(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_1): Linear(in_features=512, out_features=2048, bias=True)\n          (layer_2): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (addnorm_1): AddAndNorm(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm()\n        )\n        (addnorm_2): AddAndNorm(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm()\n        )\n        (addnorm_3): AddAndNorm(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm()\n        )\n      )\n    )\n    (layer_norm): LayerNorm()\n  )\n  (projection_layer): ProjectionLayer(\n    (projection_layer): Linear(in_features=512, out_features=30000, bias=True)\n  )\n)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Training and Validation of HindiGPT","metadata":{}},{"cell_type":"code","source":"def run_validation(model, validation_ds, tokenizer_en, tokenizer_hi, max_seq_len, device, print_msg, global_step):\n    model.eval()\n    count = 0\n\n    with torch.no_grad():\n        for batch in validation_ds:\n            count += 1\n            encoder_input = batch[\"encoder_input\"].to(device)\n            encoder_mask = batch[\"encoder_mask\"].to(device)\n\n            cls_id = tokenizer_hi.token_to_id('[CLS]')\n            sep_id = tokenizer_hi.token_to_id('[SEP]')\n\n            encoder_output = model.encode(encoder_input, encoder_mask)\n            decoder_input = torch.empty(1, 1).fill_(cls_id).type_as(encoder_input).to(device)\n\n            while True:\n                if decoder_input.size(1) == max_seq_len:\n                    break\n\n                decoder_mask = causal_mask(decoder_input.size(1)).type_as(encoder_mask).to(device)\n\n                out = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n\n                prob = model.project(out[:, -1])\n\n                _, next_word = torch.max(prob, dim=1)\n                decoder_input = torch.cat(\n                    [decoder_input, torch.empty(1, 1).type_as(encoder_input).fill_(next_word.item()).to(device)], dim=1\n                )\n\n                if next_word == sep_id:\n                    break\n\n            model_out = decoder_input.squeeze(0)\n\n            source_text = batch[\"source_text\"][0]\n            target_text = batch[\"target_text\"][0]\n            model_out_text = tokenizer_hi.decode(model_out.detach().cpu().numpy())\n\n            print_msg('-'*55)\n            print_msg(f'Source Text: {source_text}')\n            print_msg(f'Target Text: {target_text}')\n            print_msg(f'Predicted by HindiGPT: {model_out_text}')\n\n            if count == 2:\n                break\n\ndef train_model(preload_epoch=None):\n    EPOCHS = 10\n    initial_epoch = 0\n    global_step = 0\n    total_loss = 0\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, eps=1e-9)\n\n    if preload_epoch is not None:\n      model_filename = f\"./hindigpt/model_{preload_epoch}.pt\"\n      state = torch.load(model_filename)\n      model.load_state_dict(state['model_state_dict'])\n      initial_epoch = state['epoch'] + 1\n      optimizer.load_state_dict(state['optimizer_state_dict'])\n      global_step = state['global_step']\n\n    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_en.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n\n    for epoch in range(initial_epoch, EPOCHS):\n        model.train()\n        epoch_loss = 0\n        batch_count = 0\n\n        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n        for batch in batch_iterator:\n            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n            target_label = batch['target_label'].to(device) # (B, seq_len)\n\n            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n            projection_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n\n            # Compute the loss using a simple cross entropy\n            loss = loss_fn(projection_output.view(-1, tokenizer_hi.get_vocab_size()), target_label.view(-1))\n            \n            # Track total loss\n            epoch_loss += loss.item()\n            batch_count += 1\n\n            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n\n            # Backpropagate the loss\n            loss.backward()\n\n            # Update the weights\n            optimizer.step()\n            optimizer.zero_grad(set_to_none=True)\n\n            global_step += 1\n\n        # Log epoch loss\n        avg_loss = epoch_loss / batch_count\n        print(f\"\\nEpoch {epoch}: Average Loss = {avg_loss:.4f}\")\n        \n        # Validation\n        run_validation(model, val_dataloader, tokenizer_en, tokenizer_hi, max_seq_len, device, lambda msg: batch_iterator.write(msg), global_step)\n\n        # Save the model at the end of every epoch\n        model_filename = f\"./hindigpt/model_{epoch}.pt\"\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'global_step': global_step\n        }, model_filename)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T18:45:45.383131Z","iopub.execute_input":"2025-03-24T18:45:45.383401Z","iopub.status.idle":"2025-03-24T18:45:45.395212Z","shell.execute_reply.started":"2025-03-24T18:45:45.383371Z","shell.execute_reply":"2025-03-24T18:45:45.394372Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"code","source":"train_model(preload_epoch=None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T18:45:45.396107Z","iopub.execute_input":"2025-03-24T18:45:45.396404Z","iopub.status.idle":"2025-03-25T01:02:32.254206Z","shell.execute_reply.started":"2025-03-24T18:45:45.396384Z","shell.execute_reply":"2025-03-25T01:02:32.253390Z"}},"outputs":[{"name":"stderr","text":"Processing Epoch 00: 100%|██████████| 3125/3125 [37:31<00:00,  1.39it/s, loss=5.169]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 0: Average Loss = 5.9986\n-------------------------------------------------------\nSource Text: Non-repudiation\nTarget Text: गैर-रेपुडियेशन\nPredicted by HindiGPT: - ए - पट्टी\n-------------------------------------------------------\nSource Text: It is He who made the night for you, that you may rest in it, and the day to provide visibility. There are indeed signs in that for people who listen.\nTarget Text: वह वही (खुदाए क़ादिर तवाना) है जिसने तुम्हारे नफा के वास्ते रात को बनाया ताकि तुम इसमें चैन करो और दिन को (बनाया) कि उसकी रौशनी में देखो भालो उसमें शक़ नहीं जो लोग सुन लेते हैं उनके लिए इसमें (कुदरत की बहुतेरी निशानियाँ हैं)\nPredicted by HindiGPT: और जो लोग ( ख़ुदा की ) ज़मीन में है और तुम पर ( भी ) तुम पर ( भी ) तुम पर ( भी ) तुम पर ( भी ) तुम लोगों को ( भी ) एक तरह ( और ) एक तरह ( और ) एक तरह ( और ) एक चीज़ से ( क़यामत की ) हिदायत व ज़मीन में ( भी ) है\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 01: 100%|██████████| 3125/3125 [37:43<00:00,  1.38it/s, loss=4.983]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1: Average Loss = 4.9012\n-------------------------------------------------------\nSource Text: (ILIR'S BROTHER SPEAKING ALBANIAN ON VIDEO)\nTarget Text: (LLIR के भाई वीडियो पर अल्बानियाई बोल)\nPredicted by HindiGPT: ( c ) 2003 , [ ] ]\n-------------------------------------------------------\nSource Text: Say, ‘Invoke those whom you claim [to be gods] besides Him. They have no power to remove your distress, nor to bring about any change [in your state].\nTarget Text: कह दो, \"तुम उससे इतर जिनको भी पूज्य-प्रभु समझते हो उन्हें पुकार कर देखो। वे न तुमसे कोई कष्ट दूर करने का अधिकार रखते है और न उसे बदलने का।\"\nPredicted by HindiGPT: कहो , \" क्या तुम अल्लाह के मार्ग में से हटकर जो कुछ वे करते हो , वे अपने ही को नहीं मानते , और न तुम अपने ही को कोई सहायक न पाओगे\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 02: 100%|██████████| 3125/3125 [37:37<00:00,  1.38it/s, loss=4.083]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2: Average Loss = 4.2738\n-------------------------------------------------------\nSource Text: A line to be used as a separator\nTarget Text: विभाजक के रूप में इस्तेमाल करने के लिए एक लकीर\nPredicted by HindiGPT: एक पंक्ति के रूप में एक पंक्ति के लिए पंक्ति\n-------------------------------------------------------\nSource Text: That I would see it coming...\nTarget Text: मैं यह आ रहा देखना होगा कि ...\nPredicted by HindiGPT: मैं इसे देख रहा हूँ ...\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 03: 100%|██████████| 3125/3125 [37:40<00:00,  1.38it/s, loss=3.667]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3: Average Loss = 3.7850\n-------------------------------------------------------\nSource Text: The Naushera SDM assured the disabled that their problems could be resolved.\nTarget Text: एसडीएम नौशहरा ने विकलांगों को आश्वासन दिया कि उनकी समस्याएं हल हो सकती हैं।\nPredicted by HindiGPT: कोई आरंभीकरण प्रविष्टि नहीं है जो अपनी सेवा से उन्हें बदल सकते हैं .\n-------------------------------------------------------\nSource Text: But beware of men who think too much.\nTarget Text: लेकिन बहुत ज्यादा लगता है, जो पुरुषों से सावधान रहना.\nPredicted by HindiGPT: लेकिन लोगों को सुना है जो बहुत बहुत से .\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 04: 100%|██████████| 3125/3125 [37:36<00:00,  1.39it/s, loss=3.610]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4: Average Loss = 3.3860\n-------------------------------------------------------\nSource Text: Whether to show contacts that are offline in the contact list.\nTarget Text: क्या संपर्कों को दिखाना है जो संपर्क सूची में ऑफ़लाइन हैं.\nPredicted by HindiGPT: क्या संपर्क सूची में संपर्क पूर्वावलोकन दिखाता है .\n-------------------------------------------------------\nSource Text: & Help\nTarget Text: मारबल डेस्कटॉप ग्लोब के बारे में (A)\nPredicted by HindiGPT: मदद ( C )\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 05: 100%|██████████| 3125/3125 [37:38<00:00,  1.38it/s, loss=2.819]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5: Average Loss = 3.0587\n-------------------------------------------------------\nSource Text: [ CAMERON CHUCKLES ]\nTarget Text: [कैमरून दूसरे से टकराए]\nPredicted by HindiGPT: [ टीवी पर ]: पर ]:\n-------------------------------------------------------\nSource Text: - JOB DONE, EH?\nTarget Text: - काम किया, एह?\nPredicted by HindiGPT: - किसी को , एह , एह\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 06: 100%|██████████| 3125/3125 [37:36<00:00,  1.39it/s, loss=2.662]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6: Average Loss = 2.7936\n-------------------------------------------------------\nSource Text: Cartago\nTarget Text: कार्तागोcosta_ rica. kgm\nPredicted by HindiGPT: कार puertorico . kgm\n-------------------------------------------------------\nSource Text: - What?\nTarget Text: - क्या?\nPredicted by HindiGPT: - क्या ?\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 07: 100%|██████████| 3125/3125 [37:40<00:00,  1.38it/s, loss=2.570]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7: Average Loss = 2.5818\n-------------------------------------------------------\nSource Text: Satan has got the better of them and has caused them to forget the remembrance of God. They have gone over to the side of the devil, and it is as the devil's partisans that they shall be the losers:\nTarget Text: उनपर शैतान ने पूरी तरह अपना प्रभाव जमा लिया है। अतः उसने अल्लाह की याद को उनसे भुला दिया। वे शैतान की पार्टीवाले हैं। सावधान रहो शैतान की पार्टीवाले ही घाटे में रहनेवाले हैं!\nPredicted by HindiGPT: शैतान ने उनके वास्ते अच्छा कर दिखाया और उन्हें अल्लाह की याद भुला दी , ये शैतान की याद के मामले में वे शैतान का ख़याल डाल रहे है । निश्चय ही वे ज़ालिम होंगे\n-------------------------------------------------------\nSource Text: Robert Brogden, sr., our daddy, was sheriff for two decades before him.\nTarget Text: रॉबर्ट Brogden, सीनियर, हमारे पिताजी, उसे पहले दो दशकों के लिए था शेरिफ.\nPredicted by HindiGPT: रॉबर्ट परिवहन कर्ता , हमारे पिता के लिए , हमारी नौकरी के लिए खुद की आवश्यकता थी .\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 08: 100%|██████████| 3125/3125 [37:40<00:00,  1.38it/s, loss=2.312]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8: Average Loss = 2.4134\n-------------------------------------------------------\nSource Text: I will give you an answer in a day or two.\nTarget Text: मैं तुम्हें एक-दो दिन में जवाब दूँगा।\nPredicted by HindiGPT: मैं तुम्हें दो दिन या दो दिन में एक जवाब देना होगा .\n-------------------------------------------------------\nSource Text: It's three years since Mama Bengta died. It's three years since Mama Bengta died.\nTarget Text: तीन साल हो गये मेरे मम्मा बैंग्टा को मरे.\nPredicted by HindiGPT: यह तीन साल मर चुका है ... ... ... ... / मैं ... ... / मैं ... ... / मैं\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 09: 100%|██████████| 3125/3125 [37:38<00:00,  1.38it/s, loss=2.445]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9: Average Loss = 2.2791\n-------------------------------------------------------\nSource Text: Come back when you've got the 10 grand, that's my minimum.\nTarget Text: आप 10 हज़ार मिल गया है जब वापस आओ, कि मेरी न्यूनतम है.\nPredicted by HindiGPT: वापस आओ कि आप 10 , मेरे न्यूनतम अंक है कि .\n-------------------------------------------------------\nSource Text: - OH MAN.\nTarget Text: - अरे यार.\nPredicted by HindiGPT: - ओह आदमी .\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# Testing the HindiGPT model to translate new sentences","metadata":{}},{"cell_type":"code","source":"def hindigpt(user_input_text):\n\n    user_input_text = str(user_input_text).strip()\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    tokenizer_en = Tokenizer.from_file(\"./tokenizer_en/tokenizer_en.json\")\n    tokenizer_my = Tokenizer.from_file(\"./tokenizer_hi/tokenizer_hi.json\")\n\n    model = build_model(tokenizer_en.get_vocab_size(), tokenizer_hi.get_vocab_size(),max_seq_len, max_seq_len, d_model=512).to(device)\n\n    checkpoint_number = 9\n    model_filename = f\"./hindigpt/model_{checkpoint_number}.pt\"\n    state = torch.load(model_filename)\n    model.load_state_dict(state['model_state_dict'])\n\n    model.eval()\n    with torch.no_grad():\n      \n        source_text_encoding = tokenizer_en.encode(user_input_text)\n        source_text_encoding = torch.cat([\n            torch.tensor([tokenizer_en.token_to_id('[CLS]')], dtype=torch.int64),\n            torch.tensor(source_text_encoding.ids, dtype=torch.int64),\n            torch.tensor([tokenizer_en.token_to_id('[SEP]')], dtype=torch.int64),\n            torch.tensor([tokenizer_en.token_to_id('[PAD]')] * (max_seq_len - len(source_text_encoding.ids) - 2), dtype=torch.int64)\n        ], dim=0).to(device)\n        source_mask = (source_text_encoding != tokenizer_en.token_to_id('[PAD]')).unsqueeze(0).unsqueeze(0).int().to(device)\n        encoder_output = model.encode(source_text_encoding, source_mask)\n\n        decoder_input = torch.empty(1, 1).fill_(tokenizer_hi.token_to_id('[CLS]')).type_as(source_text_encoding).to(device)\n\n        # Generating the translation word by word\n        while decoder_input.size(1) < max_seq_len:\n            decoder_mask = torch.triu(torch.ones((1, decoder_input.size(1), decoder_input.size(1))), diagonal=1).type(torch.int).type_as(source_mask).to(device)\n            out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n\n            prob = model.project(out[:, -1])\n            _, next_word = torch.max(prob, dim=1)\n            decoder_input = torch.cat([decoder_input, torch.empty(1, 1).type_as(source_text_encoding).fill_(next_word.item()).to(device)], dim=1)\n\n            if next_word == tokenizer_hi.token_to_id('[SEP]'):\n                break\n\n    # Converting ids to tokens\n    return tokenizer_hi.decode(decoder_input[0].tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T01:02:32.255530Z","iopub.execute_input":"2025-03-25T01:02:32.256110Z","iopub.status.idle":"2025-03-25T01:02:32.264470Z","shell.execute_reply.started":"2025-03-25T01:02:32.256084Z","shell.execute_reply":"2025-03-25T01:02:32.263695Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Translation using HindiGPT\n## TEST 1","metadata":{}},{"cell_type":"code","source":"user_input = \"Good Morning\"\ntransalated_text = hindigpt(user_input)\n\nprint(f\"User input (in English): {user_input}\")\nprint(f\"Translation (in Hindi): {transalated_text}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T08:20:03.561219Z","iopub.execute_input":"2025-03-25T08:20:03.561578Z","iopub.status.idle":"2025-03-25T08:20:03.565099Z","shell.execute_reply.started":"2025-03-25T08:20:03.561551Z","shell.execute_reply":"2025-03-25T08:20:03.564227Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## TEST 2","metadata":{}},{"cell_type":"code","source":"user_input = \"How are you?\"\ntransalated_text = hindigpt(user_input)\n\nprint(f\"User input (in English): {user_input}\")\nprint(f\"Translation (in Hindi): {transalated_text}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T08:23:19.575699Z","iopub.execute_input":"2025-03-25T08:23:19.576056Z","iopub.status.idle":"2025-03-25T08:23:19.579436Z","shell.execute_reply.started":"2025-03-25T08:23:19.576031Z","shell.execute_reply":"2025-03-25T08:23:19.578666Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## TEST 3","metadata":{}},{"cell_type":"code","source":"user_input = \"Hi, I am good.\"\ntransalated_text = hindigpt(user_input)\n\nprint(f\"User input (in English): {user_input}\")\nprint(f\"Translation (in Hindi): {transalated_text}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T08:23:38.933058Z","iopub.execute_input":"2025-03-25T08:23:38.933362Z","iopub.status.idle":"2025-03-25T08:23:38.936960Z","shell.execute_reply.started":"2025-03-25T08:23:38.933338Z","shell.execute_reply":"2025-03-25T08:23:38.936070Z"}},"outputs":[],"execution_count":5}]}